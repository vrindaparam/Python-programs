{"cells":[{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["import re"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["string = \"tiger is national animal of india\"\n","pattern = \"india\""]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<re.Match object; span=(28, 33), match='india'>\n"]}],"source":["mo = re.search(pattern,string)\n","print(mo)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<re.Match object; span=(0, 5), match='tiger'>\n"]}],"source":["pattern2 = \"tiger\"\n","mo = re.match(pattern2, string)\n","print(mo)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["None\n"]}],"source":["mo = re.match(pattern, string)\n","print(mo)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['national', 'national']\n"]}],"source":["string = \"tiger is national animal of india and national sports of hockey\"\n","pattern = \"national\"\n","\n","mo = re.findall(pattern, string)\n","print(mo)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["9\n","38\n"]}],"source":["mo = re.finditer(pattern, string)\n","for m in mo:\n","    print(m.start())\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tiger is peacock animal of india and peacock sports of hockey\n"]}],"source":["print(re.sub(pattern, \"peacock\", string))"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/plain":["<re.Match object; span=(6, 10), match='goal'>"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["sent = \"Final goal was scored by captain of the team\"\n","pattern = \"goal\"\n","\n","re.search(pattern, sent)"]},{"cell_type":"markdown","metadata":{},"source":["IMplementing text pre-processing using NLTK"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/plain":["['Hi John, How are you doing ?',\n"," 'I will be travalling to your city.',\n"," 'lets catchup']"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.tokenize import sent_tokenize, word_tokenize\n","text = \"Hi John, How are you doing ? I will be travalling to your city. lets catchup \"\n","\n","#List of sentences\n","sent_tokenize(text)\n","\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["['Hi',\n"," 'John',\n"," ',',\n"," 'How',\n"," 'are',\n"," 'you',\n"," 'doing',\n"," '?',\n"," 'I',\n"," 'will',\n"," 'be',\n"," 'travalling',\n"," 'to',\n"," 'your',\n"," 'city',\n"," '.',\n"," 'lets',\n"," 'catchup']"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["#list of words\n","word_tokenize(text)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["play\n"]}],"source":["#stemming\n","from nltk.stem import PorterStemmer\n","\n","stemmer = PorterStemmer()\n","\n","print(stemmer.stem('playing'))"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["increase\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package omw-1.4 to /Users/vrinda/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}],"source":["\n","import nltk\n","nltk.download('omw-1.4')\n","from nltk.stem import WordNetLemmatizer\n","\n","lemm = WordNetLemmatizer()\n","\n","print(lemm.lemmatize(\"increases\"))"]},{"cell_type":"markdown","metadata":{},"source":["To get POS tags from a String "]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Output of work_tokenize\n"," ['Hi', 'John', ',', 'how', 'are', 'you', 'doing', '?', 'I', '`', 'll', 'be', 'travelling', 'to', 'your', 'city', ',', 'lets', 'catchup']\n","\n"," POS tags of a sentence\n","\n"]},{"data":{"text/plain":["[('Hi', 'NNP'),\n"," ('John', 'NNP'),\n"," (',', ','),\n"," ('how', 'WRB'),\n"," ('are', 'VBP'),\n"," ('you', 'PRP'),\n"," ('doing', 'VBG'),\n"," ('?', '.'),\n"," ('I', 'PRP'),\n"," ('`', '``'),\n"," ('ll', 'RB'),\n"," ('be', 'VB'),\n"," ('travelling', 'VBG'),\n"," ('to', 'TO'),\n"," ('your', 'PRP$'),\n"," ('city', 'NN'),\n"," (',', ','),\n"," ('lets', 'VBZ'),\n"," ('catchup', 'NN')]"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["from nltk import pos_tag\n","text = \"Hi John, how are you doing ? I`ll be travelling to your city, lets catchup\"\n","\n","token = word_tokenize(text)\n","print(\"Output of work_tokenize\\n\", token)\n","print(\"\\n POS tags of a sentence\\n\")\n","pos_tag(token)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/plain":["[Synset('computer.n.01'), Synset('calculator.n.01')]"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.corpus import wordnet\n","wordnet.synsets('computer')"]},{"cell_type":"markdown","metadata":{},"source":["To generate Bigrams from a String using NLTK "]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/plain":["<zip at 0x147ce8dc0>"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["from nltk import ngrams\n","sent = \"hello , have a great day\"\n","\n","n = 2\n","ngrams(word_tokenize(sent), n)\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('hello', ',')\n","(',', 'have')\n","('have', 'a')\n","('a', 'great')\n","('great', 'day')\n"]}],"source":["for gram in ngrams(word_tokenize(sent), n):\n","    print(gram)"]},{"cell_type":"markdown","metadata":{},"source":["To get trigrams from a string using NLTK "]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('hello', ',', 'have')\n","(',', 'have', 'a')\n","('have', 'a', 'great')\n","('a', 'great', 'day')\n"]}],"source":["from nltk import ngrams\n","sent = \"hello , have a great day\"\n","\n","n = 3\n","ngrams(word_tokenize(sent), n)\n","for gram in ngrams(word_tokenize(sent), n):\n","    print(gram)"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('hello',)\n","(',',)\n","('have',)\n","('a',)\n","('great',)\n","('day',)\n"]}],"source":["from nltk import ngrams\n","sent = \"hello , have a great day\"\n","\n","n = 1\n","ngrams(word_tokenize(sent), n)\n","for gram in ngrams(word_tokenize(sent), n):\n","    print(gram)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["25\n"]}],"source":["Text= \"I like eating fruits such as apple, kiwi, orange, and guava. Also, I like eating fast foods like Pizza.\"\n","tokens = word_tokenize(Text)\n","print(len(tokens))"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('India', 'won', 'the')\n","('won', 'the', 'match')\n","('the', 'match', 'today')\n"]}],"source":["str = \"India won the match today\"\n","n = 3\n","ngrams(word_tokenize(str), n)\n","for gram in ngrams(word_tokenize(str), n):\n","    print(gram)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('new', 'Delhi')\n","('Delhi', 'is')\n","('is', 'the')\n","('the', 'capital')\n","('capital', 'of')\n","('of', 'India')\n"]}],"source":["str = \"new Delhi is the capital of India\"\n","n = 2\n","ngrams(word_tokenize(str), n)\n","for gram in ngrams(word_tokenize(str), n):\n","    print(gram)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":2}
